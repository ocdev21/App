---
# Namespace for OpenShift AI
apiVersion: v1
kind: Namespace
metadata:
  name: l1-app-ai
  labels:
    name: l1-app-ai
    purpose: l1-troubleshooting-ai-platform
    opendatahub.io/dashboard: 'true'
---
# L1 Application ConfigMap for OpenShift AI
apiVersion: v1
kind: ConfigMap
metadata:
  name: l1-app-ai-config
  namespace: l1-app-ai
data:
  NODE_ENV: "production"
  PORT: "5000"
  CLICKHOUSE_URL: "http://clickhouse-clickhouse-single:8123"
  CLICKHOUSE_HOST: "clickhouse-clickhouse-single"
  CLICKHOUSE_PORT: "8123"
  CLICKHOUSE_DATABASE: "l1_anomaly_detection"
  CLICKHOUSE_USER: "default"
  CLICKHOUSE_USERNAME: "default"
  CLICKHOUSE_PASSWORD: "defaultpass"
  TSLAM_REMOTE_HOST: "10.193.0.4"
  TSLAM_REMOTE_PORT: "8080"
  # OpenShift AI Model Serving Endpoints
  RHOAI_MODEL_SERVING: "false"
  RHOAI_INFERENCE_ENDPOINT: "http://tslam-model-service.l1-app-ai.svc.cluster.local:8080/predict"
  JUPYTER_SERVICE_URL: "http://jupyter-service.l1-app-ai.svc.cluster.local:8888"
---
# L1 Application Secrets
apiVersion: v1
kind: Secret
metadata:
  name: l1-app-ai-secrets
  namespace: l1-app-ai
type: Opaque
data:
  database_password: ""
  jwt_secret: bDEtYXBwLWp3dC1zZWNyZXQ=
  model_api_key: ""
  openshift_ai_token: ""
---
# ClickHouse PVC - using default storage class
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: clickhouse-ai-pvc
  namespace: l1-app-ai
spec:
  accessModes:
    - ReadWriteOnce
  volumeMode: Filesystem
  resources:
    requests:
      storage: 10Gi
---
# L1 Application Data PVC - using default storage class
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: l1-app-ai-data-pvc
  namespace: l1-app-ai
spec:
  accessModes:
    - ReadWriteOnce
  volumeMode: Filesystem
  resources:
    requests:
      storage: 10Gi
---
# Enhanced ML Models PVC for TSLAM-4B and ensemble models
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: l1-ml-models-pvc
  namespace: l1-app-ai
  labels:
    app: l1-troubleshooting
    component: ml-storage
    model-type: tslam-4b
    usage: gpu-inference
spec:
  accessModes:
    - ReadWriteMany  # Multiple GPU pods can read the same model files
  resources:
    requests:
      storage: 20Gi   # 20GB storage for TSLAM-4B (8-15GB) + ensemble models
  storageClassName: ""  # Uses default storage class
  volumeMode: Filesystem
---
# ClickHouse Deployment for AI Platform
apiVersion: apps/v1
kind: Deployment
metadata:
  name: clickhouse-ai
  namespace: l1-app-ai
  labels:
    app: clickhouse-ai
spec:
  replicas: 1
  selector:
    matchLabels:
      app: clickhouse-ai
  template:
    metadata:
      labels:
        app: clickhouse-ai
    spec:
      initContainers:
      - name: setup-clickhouse-dirs
        image: busybox:1.35
        command: ['sh', '-c']
        args:
        - |
          # Create all necessary directories (ownership is already correct as user 1001)
          mkdir -p /clickhouse-data/data
          mkdir -p /clickhouse-data/logs
          mkdir -p /clickhouse-data/tmp
          mkdir -p /clickhouse-data/user_files
          mkdir -p /clickhouse-data/access
          mkdir -p /clickhouse-data/flags
          mkdir -p /clickhouse-data/format_schemas
          mkdir -p /clickhouse-data/preprocessed_configs
          mkdir -p /clickhouse-data/user_defined

          # Set permissions (can't use chown without root, but mkdir creates with correct ownership)
          chmod -R 755 /clickhouse-data 2>/dev/null || true
          
          echo "ClickHouse directories created successfully"
        volumeMounts:
        - name: clickhouse-data
          mountPath: /clickhouse-data
        securityContext:
          runAsNonRoot: true
          runAsUser: 1001
          runAsGroup: 1001
          allowPrivilegeEscalation: false
      containers:
      - name: clickhouse
        image: clickhouse/clickhouse-server:23.8
        imagePullPolicy: IfNotPresent
        securityContext:
          runAsNonRoot: true
          runAsUser: 1001
          runAsGroup: 1001
          allowPrivilegeEscalation: false
        ports:
        - name: http
          containerPort: 9000
        - name: tcp
          containerPort: 9000
        env:
        - name: CLICKHOUSE_DB
          value: "l1_anomaly_detection"
        - name: CLICKHOUSE_USER
          value: "default"
        - name: CLICKHOUSE_PASSWORD
          value: ""
        - name: CLICKHOUSE_DATA_DIR
          value: "/var/lib/clickhouse/"
        - name: CLICKHOUSE_LOG_DIR
          value: "/var/lib/clickhouse/logs/"
        - name: CLICKHOUSE_TMP_DIR
          value: "/var/lib/clickhouse/tmp/"
        volumeMounts:
        - name: clickhouse-data
          mountPath: /var/lib/clickhouse
        resources:
          requests:
            memory: "4Gi"
            cpu: "2000m"
          limits:
            memory: "8Gi"
            cpu: "4000m"
        livenessProbe:
          httpGet:
            path: /ping
            port: 9000
          initialDelaySeconds: 30
          periodSeconds: 30
        readinessProbe:
          httpGet:
            path: /ping
            port: 9000
          initialDelaySeconds: 10
          periodSeconds: 10
      volumes:
      - name: clickhouse-data
        persistentVolumeClaim:
          claimName: clickhouse-ai-pvc
---
# ClickHouse Service
apiVersion: v1
kind: Service
metadata:
  name: clickhouse-service
  namespace: l1-app-ai
  labels:
    app: clickhouse-ai
spec:
  selector:
    app: clickhouse-ai
  ports:
  - name: http
    protocol: TCP
    port: 9000
    targetPort: 9000
  - name: tcp
    protocol: TCP
    port: 9000
    targetPort: 9000
  type: ClusterIP
---
# Model Upload Job for TSLAM-4B
apiVersion: v1
kind: Pod
metadata:
  name: tslam-model-uploader
  namespace: l1-app-ai
  labels:
    app: model-uploader
    model: tslam-4b
spec:
  restartPolicy: Never
  containers:
  - name: uploader
    image: busybox:1.35
    command: ["sleep", "3600"]  # Stays alive for 1 hour for manual upload
    volumeMounts:
    - name: model-storage
      mountPath: /models
  volumes:
  - name: model-storage
    persistentVolumeClaim:
      claimName: l1-ml-models-pvc
---
# TSLAM vLLM GPU Inference Deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: tslam-vllm-deployment
  namespace: l1-app-ai
  labels:
    app: tslam-vllm
    model: tslam-4b
    opendatahub.io/dashboard: 'true'
spec:
  replicas: 3  # One per GPU node
  selector:
    matchLabels:
      app: tslam-vllm
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0
  template:
    metadata:
      labels:
        app: tslam-vllm
        model: tslam-4b
    spec:
      serviceAccountName: l1-app-ai-sa
      # Ensure pods are distributed across GPU nodes
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: kubernetes.io/hostname
                operator: In
                values:
                - rhocp-gx5wg-worker-0-vfm8l
                - rhocp-gx5wg-worker-0-pdg59
                - rhocp-gx5wg-worker-0-cbmkw
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchLabels:
                app: tslam-vllm
            topologyKey: kubernetes.io/hostname
      containers:
      - name: vllm-server
        image: vllm/vllm-openai:v0.6.2
        ports:
        - name: http
          containerPort: 8000
        command:
        - python
        - -m
        - vllm.entrypoints.openai.api_server
        args:
        - --model
        - /models/tslam-4b
        - --host
        - 0.0.0.0
        - --port
        - "8000"
        - --tensor-parallel-size
        - "1"
        - --max-model-len
        - "2048"
        - --gpu-memory-utilization
        - "0.8"
        - --served-model-name
        - tslam-4b
        env:
        - name: CUDA_VISIBLE_DEVICES
          value: "0"
        - name: VLLM_WORKER_MULTIPROC_METHOD
          value: spawn
        volumeMounts:
        - name: model-storage
          mountPath: /models
          readOnly: true
        - name: tmp-storage
          mountPath: /tmp
        resources:
          requests:
            nvidia.com/gpu: 1
            memory: "8Gi"
            cpu: "4000m"
          limits:
            nvidia.com/gpu: 1
            memory: "16Gi"
            cpu: "8000m"
        livenessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 120
          periodSeconds: 30
          timeoutSeconds: 10
        readinessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 60
          periodSeconds: 10
          timeoutSeconds: 5
      volumes:
      - name: model-storage
        persistentVolumeClaim:
          claimName: l1-ml-models-pvc
      - name: tmp-storage
        emptyDir:
          sizeLimit: 4Gi
---
# TSLAM vLLM Load Balancer Service
apiVersion: v1
kind: Service
metadata:
  name: tslam-vllm-service
  namespace: l1-app-ai
  labels:
    app: tslam-vllm
    model: tslam-4b
    service-type: gpu-inference
spec:
  selector:
    app: tslam-vllm
  ports:
  - name: http
    protocol: TCP
    port: 8000
    targetPort: 8000
  type: ClusterIP
  sessionAffinity: None  # Load balance across all GPU nodes
---
# ConfigMap with basic application code
apiVersion: v1
kind: ConfigMap
metadata:
  name: l1-app-code-config
  namespace: l1-app-ai
data:
  package.json: |
    {
      "name": "l1-troubleshooting-ai",
      "version": "1.0.0",
      "main": "server.js",
      "scripts": {
        "start": "node server.js"
      },
      "dependencies": {
        "express": "^4.18.0",
        "cors": "^2.8.5"
      }
    }
  server.js: |
    const express = require('express');
    const cors = require('cors');
    const app = express();
    const port = process.env.PORT || 5000;

    app.use(cors());
    app.use(express.json());

    app.get('/health', (req, res) => {
      res.json({ status: 'healthy', timestamp: new Date().toISOString() });
    });

    app.get('/ready', (req, res) => {
      res.json({ status: 'ready', timestamp: new Date().toISOString() });
    });

    app.get('/', (req, res) => {
      res.json({
        message: 'L1 Troubleshooting AI Application',
        version: '1.0.0',
        environment: process.env.NODE_ENV || 'production',
        clickhouse: process.env.CLICKHOUSE_URL,
        timestamp: new Date().toISOString()
      });
    });

    app.listen(port, '0.0.0.0', () => {
      console.log(`L1 Troubleshooting AI server running on port ${port}`);
    });
  Dockerfile: |
    FROM node:18-alpine
    WORKDIR /app
    COPY package*.json ./
    RUN npm install --production
    COPY . .
    EXPOSE 5000
    USER 1001
    CMD ["npm", "start"]
---
# L1 Application BuildConfig for OpenShift AI
apiVersion: build.openshift.io/v1
kind: BuildConfig
metadata:
  name: l1-app-ai-build
  namespace: l1-app-ai
  labels:
    app: l1-troubleshooting-ai
    opendatahub.io/dashboard: 'true'
spec:
  source:
    type: Dockerfile
    dockerfile: |
      FROM node:18-alpine
      WORKDIR /app
      COPY package*.json ./
      RUN npm install --production
      COPY . .
      EXPOSE 5000
      USER 1001
      CMD ["npm", "start"]
    configMaps:
    - configMap:
        name: l1-app-code-config
      destinationDir: .
  strategy:
    type: Docker
    dockerStrategy: {}
  output:
    to:
      kind: ImageStreamTag
      name: l1-troubleshooting-ai:latest
  triggers:
  - type: ConfigChange
---
# L1 Application ImageStream
apiVersion: image.openshift.io/v1
kind: ImageStream
metadata:
  name: l1-troubleshooting-ai
  namespace: l1-app-ai
  labels:
    app: l1-troubleshooting-ai
    opendatahub.io/dashboard: 'true'
spec:
  lookupPolicy:
    local: false
---
# Service Account for OpenShift AI
apiVersion: v1
kind: ServiceAccount
metadata:
  name: l1-app-ai-sa
  namespace: l1-app-ai
  labels:
    app: l1-troubleshooting-ai
    opendatahub.io/dashboard: 'true'
---
# Role for OpenShift AI Integration
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: l1-app-ai-role
  namespace: l1-app-ai
rules:
- apiGroups: [""]
  resources: ["pods", "services", "configmaps", "secrets"]
  verbs: ["get", "list", "watch"]
- apiGroups: ["apps"]
  resources: ["deployments", "replicasets"]
  verbs: ["get", "list", "watch"]
- apiGroups: ["route.openshift.io"]
  resources: ["routes"]
  verbs: ["get", "list", "watch"]
---
# RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: l1-app-ai-rolebinding
  namespace: l1-app-ai
subjects:
- kind: ServiceAccount
  name: l1-app-ai-sa
  namespace: l1-app-ai
roleRef:
  kind: Role
  name: l1-app-ai-role
  apiGroup: rbac.authorization.k8s.io
---
# L1 Application Deployment for OpenShift AI
apiVersion: apps/v1
kind: Deployment
metadata:
  name: l1-troubleshooting-ai
  namespace: l1-app-ai
  labels:
    app: l1-troubleshooting-ai
    version: v1.0.0
    opendatahub.io/dashboard: 'true'
spec:
  replicas: 2
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0
  selector:
    matchLabels:
      app: l1-troubleshooting-ai
  template:
    metadata:
      labels:
        app: l1-troubleshooting-ai
        version: v1.0.0
    spec:
      serviceAccountName: l1-app-ai-sa
      containers:
      - name: l1-app-ai
        image: image-registry.openshift-image-registry.svc:5000/l1-app-ai/l1-troubleshooting-ai:latest
        imagePullPolicy: Always
        ports:
        - name: http
          containerPort: 5000
          protocol: TCP
        env:
        - name: PORT
          valueFrom:
            configMapKeyRef:
              name: l1-app-ai-config
              key: PORT
        - name: NODE_ENV
          valueFrom:
            configMapKeyRef:
              name: l1-app-ai-config
              key: NODE_ENV
        - name: CLICKHOUSE_URL
          valueFrom:
            configMapKeyRef:
              name: l1-app-ai-config
              key: CLICKHOUSE_URL
        - name: CLICKHOUSE_HOST
          valueFrom:
            configMapKeyRef:
              name: l1-app-ai-config
              key: CLICKHOUSE_HOST
        - name: CLICKHOUSE_PORT
          valueFrom:
            configMapKeyRef:
              name: l1-app-ai-config
              key: CLICKHOUSE_PORT
        - name: CLICKHOUSE_DATABASE
          valueFrom:
            configMapKeyRef:
              name: l1-app-ai-config
              key: CLICKHOUSE_DATABASE
        - name: CLICKHOUSE_USER
          valueFrom:
            configMapKeyRef:
              name: l1-app-ai-config
              key: CLICKHOUSE_USER
        - name: CLICKHOUSE_USERNAME
          valueFrom:
            configMapKeyRef:
              name: l1-app-ai-config
              key: CLICKHOUSE_USERNAME
        - name: CLICKHOUSE_PASSWORD
          valueFrom:
            configMapKeyRef:
              name: l1-app-ai-config
              key: CLICKHOUSE_PASSWORD
        - name: TSLAM_REMOTE_HOST
          valueFrom:
            configMapKeyRef:
              name: l1-app-ai-config
              key: TSLAM_REMOTE_HOST
        - name: TSLAM_REMOTE_PORT
          valueFrom:
            configMapKeyRef:
              name: l1-app-ai-config
              key: TSLAM_REMOTE_PORT
        - name: JUPYTER_SERVICE_URL
          valueFrom:
            configMapKeyRef:
              name: l1-app-ai-config
              key: JUPYTER_SERVICE_URL
        - name: RHOAI_MODEL_SERVING
          valueFrom:
            configMapKeyRef:
              name: l1-app-ai-config
              key: RHOAI_MODEL_SERVING
        - name: RHOAI_INFERENCE_ENDPOINT
          valueFrom:
            configMapKeyRef:
              name: l1-app-ai-config
              key: RHOAI_INFERENCE_ENDPOINT
        - name: JWT_SECRET
          valueFrom:
            secretKeyRef:
              name: l1-app-ai-secrets
              key: jwt_secret
        - name: MODEL_API_KEY
          valueFrom:
            secretKeyRef:
              name: l1-app-ai-secrets
              key: model_api_key
        volumeMounts:
        - name: app-data
          mountPath: /app/data
        - name: tmp-storage
          mountPath: /tmp
        - name: ml-models-storage
          mountPath: /app/models
        - name: app-code
          mountPath: /app
        resources:
          requests:
            memory: "4Gi"
            cpu: "2000m"
          limits:
            memory: "8Gi"
            cpu: "4000m"
        livenessProbe:
          httpGet:
            path: /health
            port: 5000
            scheme: HTTP
          initialDelaySeconds: 30
          periodSeconds: 30
          timeoutSeconds: 10
          failureThreshold: 3
          successThreshold: 1
        readinessProbe:
          httpGet:
            path: /ready
            port: 5000
            scheme: HTTP
          initialDelaySeconds: 10
          periodSeconds: 10
          timeoutSeconds: 5
          failureThreshold: 3
          successThreshold: 1
        startupProbe:
          httpGet:
            path: /health
            port: 5000
            scheme: HTTP
          initialDelaySeconds: 10
          periodSeconds: 10
          timeoutSeconds: 5
          failureThreshold: 30
          successThreshold: 1
      volumes:
      - name: app-data
        persistentVolumeClaim:
          claimName: l1-app-ai-data-pvc
      - name: ml-models-storage
        persistentVolumeClaim:
          claimName: l1-ml-models-pvc
      - name: tmp-storage
        emptyDir:
          sizeLimit: 2Gi
      - name: app-code
        configMap:
          name: l1-app-code-config
      restartPolicy: Always
      terminationGracePeriodSeconds: 30
---
# L1 Application Service
apiVersion: v1
kind: Service
metadata:
  name: l1-troubleshooting-ai-service
  namespace: l1-app-ai
  labels:
    app: l1-troubleshooting-ai
    opendatahub.io/dashboard: 'true'
spec:
  selector:
    app: l1-troubleshooting-ai
  ports:
  - name: http
    protocol: TCP
    port: 80
    targetPort: 5000
    nodePort: 30080
  - name: https
    protocol: TCP
    port: 443
    targetPort: 5000
    nodePort: 30443
  type: NodePort
  sessionAffinity: ClientIP
  sessionAffinityConfig:
    clientIP:
      timeoutSeconds: 3600
---
# L1 Application Route
apiVersion: route.openshift.io/v1
kind: Route
metadata:
  name: l1-troubleshooting-ai-route
  namespace: l1-app-ai
  labels:
    app: l1-troubleshooting-ai
    opendatahub.io/dashboard: 'true'
  annotations:
    haproxy.router.openshift.io/timeout: 300s
    haproxy.router.openshift.io/balance: roundrobin
spec:
  to:
    kind: Service
    name: l1-troubleshooting-ai-service
    weight: 100
  port:
    targetPort: http
  tls:
    termination: edge
    insecureEdgeTerminationPolicy: Redirect
  wildcardPolicy: None
---
# HorizontalPodAutoscaler
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: l1-troubleshooting-ai-hpa
  namespace: l1-app-ai
  labels:
    app: l1-troubleshooting-ai
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: l1-troubleshooting-ai
  minReplicas: 2
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
---
# NetworkPolicy
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: l1-app-ai-netpol
  namespace: l1-app-ai
spec:
  podSelector:
    matchLabels:
      app: l1-troubleshooting-ai
  policyTypes:
  - Ingress
  - Egress
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          name: openshift-ingress
    - namespaceSelector:
        matchLabels:
          name: l1-app-ai
    - namespaceSelector:
        matchLabels:
          name: redhat-ods-applications
    ports:
    - protocol: TCP
      port: 5000
  egress:
  - to:
    - namespaceSelector:
        matchLabels:
          name: l1-app-ai
    - namespaceSelector:
        matchLabels:
          name: redhat-ods-applications
    ports:
    - protocol: TCP
      port: 9000
    - protocol: TCP
      port: 9000
    - protocol: TCP
      port: 8080
    - protocol: TCP
      port: 8888
  - to: []
    ports:
    - protocol: TCP
      port: 53
    - protocol: UDP
      port: 53
    - protocol: TCP
      port: 443
    - protocol: TCP
      port: 80