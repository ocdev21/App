# Integrated Dockerfile - L1 Frontend/Backend + GGUF AI Inference Server
# Runs both services in a single container:
# - Port 5000: L1 Web Application (React frontend + Express backend)
# - Port 8000: AI Inference Server (Mistral GGUF with ctransformers)

FROM node:20-bullseye

WORKDIR /app

# Install system dependencies including Python for AI inference
RUN apt-get update && apt-get install -y \
    python3 \
    python3-dev \
    python3-pip \
    python3-venv \ 
    build-essential \
    gcc \
    g++ \
    libpcap-dev \
    libopenblas-dev \
    liblapack-dev \
    libatlas-base-dev \
    libhdf5-dev \
    curl \
    wget \
    supervisor \
    && rm -rf /var/lib/apt/lists/*

# Create python symlink and upgrade pip
RUN ln -sf /usr/bin/python3 /usr/bin/python \
    && python -m pip install --upgrade pip setuptools wheel

# Install Python AI dependencies for inference server + ML analysis + RAG
# Pin versions for Python 3.9 compatibility (Debian Bullseye)
RUN pip install --no-cache-dir \
    ctransformers \
    flask \
    flask-cors \
    requests \
    psutil \
    clickhouse-connect \
    scapy \
    scikit-learn \
    pandas \
    numpy \
    joblib \
    pysqlite3-binary \
    chromadb \
    sentence-transformers==2.2.2 \
    huggingface-hub==0.16.4 \
    PyPDF2

# Copy package files for dependency installation
COPY package*.json ./

# Install ALL Node.js dependencies (dev and prod)
RUN npm install

# Copy all application code
COPY . .

# Fix vite config for Docker environment
RUN sed -i 's/allowedHosts: "all"/allowedHosts: true/g' vite.config.ts || true

# Build the application (frontend + backend)
RUN npm run build

# Create directories for embedded storage
RUN mkdir -p /app/models /app/ml_data/input_files /app/ml_data/models \
    /app/chromadb /app/uploaded_docs /app/.cache

# IMPORTANT: Copy Mistral model during build (must be in build context)
# Place mistral-7b-instruct-v0.2.Q4_K_M.gguf in openshift/ directory before building
COPY mistral-7b-instruct-v0.2.Q4_K_M.gguf /app/models/mistral.gguf

# Copy GGUF inference server
COPY gguf-inference-server.py /app/gguf-server.py
RUN chmod +x /app/gguf-server.py

# Copy startup script
COPY start-services.sh /app/start-services.sh
RUN chmod +x /app/start-services.sh

# Set environment variables
ENV NODE_ENV=production
ENV PORT=5000
ENV PYTHONPATH=/app
ENV PYTHONUNBUFFERED=1
ENV PYTHONDONTWRITEBYTECODE=1
ENV ML_MODELS_DIR=/app/models
ENV INPUT_FILES_DIR=/app/ml_data/input_files
ENV FEATURE_HISTORY_DIR=/app/ml_data/feature_history
ENV CHROMADB_PERSIST_DIR=/app/chromadb
ENV UPLOADED_DOCS_DIR=/app/uploaded_docs

# Create non-root user
RUN useradd -m -u 1001 -s /bin/bash appuser \
    && chown -R appuser:appuser /app

USER appuser

# Expose all service ports
EXPOSE 5000 8000 8001

# Health check on main app
HEALTHCHECK --interval=30s --timeout=30s --start-period=120s --retries=3 \
    CMD curl -f http://localhost:5000/ || exit 1

# Start both services
CMD ["/app/start-services.sh"]
